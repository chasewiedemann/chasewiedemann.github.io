---
title: "Replication"
output: html_document
---
```{r,echo = F}
base_dir = getwd()
base_url = paste0("https://",substr(base_dir,8, nchar(base_dir)))
wp_link = paste0(base_url,"/WorkingPaper.html")
ws_link = paste0(base_url,"/WorkingSlides.html")
rep_link = paste0(base_url,"/Replication.html")
miniblog_link = paste0(base_url,"/MiniBlog.html")
```

[Home](https://chasewiedemann.github.io/index.html) |
[About](https://chasewiedemann.github.io/Site/about.html) |
[Projects](https://chasewiedemann.github.io/Site/Projects/projects.html) |
[Paper Summaries](https://chasewiedemann.github.io/Site/Summaries/summaries.html) |
[Blog](https://chasewiedemann.github.io/Site/blog.html) |

[Working Paper](`r wp_link`)|
[Working Slides](`r ws_link`)|
[Replication](`r rep_link`)|
[Mini-Blog](`r miniblog_link`)

# Models without Unobserved Heterogenity 

Consider a simple discrete choice model where the agents share a common utility function $u(X_t;\beta)$ and make choices over $J+1$ alternatives. The menu $X_t$ contains the characteristics of the alternatives in choice setting $t$ that the researcher believes effect utility and $s_t$ is a vector of choice probabilities that the researcher has collected. In all settings of interest this extremely simple model will be rejected by our data. Given that agents share a common utility function, they will all value the alternatives the same, and will all choose the same alternative to maximize utility. Contrast that with our observation that $s_t$ contains $J+1$ alternatives are present and likely has more than one positive choice probability.

The logical next step is to add an idiosyncratic preference shock to our agents utility function. This allows us to model agent $i$'s vector of utility for the alternatives as
$$ u_{it} = u(X_t;\beta) + \epsilon_i $$
Where $\epsilon_i$ is a $J\times 1$ random vector from a distribution $F_\epsilon$. Integrating out $\epsilon$ by taking an expectation over $F_\epsilon$ gives the mean utility $\delta_t$. We assume that $F_\epsilon$ is mean zero and orthogonal to the characteristics of the alternatives. Therefore we have
$$ \delta_t = \mathbb E_\epsilon\left[u_{it}\right] = u(X_t;\beta) $$
Another way to view the expectation taken over $F_\epsilon$ is through the infinite sum
$$ \delta_t = \lim_{N\to\infty} \frac{1}{N}\sum_{i=1}^N u_i + \epsilon_i $$
This form of the $\delta_t$ does not provide much additional insight at this point, but will become crucial later on.

Given a normalization, Berry (1994) shows that there is a bijection between mean utilities $\delta_t$ and any vector of choice probabilities (including the one we observe) $s_t$. This allows us to take any vector of choice probabilities $s_t$ and associate them with a unique value of $\delta_t$. We can then map $X_t$ onto $\delta_t$ to recover $\beta$.


We require a normalization because $\delta_t$ is invariant to levels. This means that $\delta_t + \alpha$ where $\alpha$ is a constant vector, provides the same amount of information about our agents evaluation of the alternatives for every choice of $\alpha$. Seen another way, the inverse mapping from choice probabilities to mean utilities is a correspondence between $s_t$ and the equivalence class given by $\delta_t +\alpha$. The normalization chooses a canonical member of each equivalence class to turn the correspondence into a bijection. 

A simple way to choose this canonical member, and therefore create a bijection that allows us to invert this model for $\delta_t$, is to normalize the mean utility of one alternative to zero.  While any of the alternatives can be normalized, the common way this is preformed is to normalize the mean utility of the outside alternative. The outside alternative is the alternative where our agent did not choose any of the other alternatives. If we were studying a choice environment regarding the market for transportation services, this alternative would capture all agents who decided to walk or bike. Let $j=0$ be the index of the outside alternative. This normalization selects exactly one member of each equivalence class because $\delta_{0t} + \alpha_0 = 0$ for exactly one value of $\alpha$. 

This bijection holds for all choices of $F_\epsilon$. However, assuming that $F_\epsilon$ is just $J$ copies of the single dimensional Type-1 Extreme Value distribution allows for this bijection to have a closed form solution given by
$$ s_{jt} = \frac{e^{\delta_{jt}}}{\sum_{k=0}^J e^{\delta_{kt}}} $$
This follows from properties of that (univariate) Type-1 Extreme Value distribution and a derivation can be found in Train (2002). Then, using our conveniently chosen normalization, we can write
$$ s_{0t} = \frac{e^{\delta_{0t}}}{\sum_{k=0}^J e^{\delta_{kt}}} = \frac{1}{\sum_{k=0}^J e^{\delta_{kt}}} $$
We see that this normalization has given us the value of the denominator of our expression. We can then divide by the value of this denominator and take the natural log to yield $\delta_{jt}$ as
$$ 
\begin{aligned}
\frac{s_{jt}}{s_{0t}} &= \frac{e^{\delta_{jt}}}{\sum_{k=0}^J e^{\delta_{kt}}} \cdot \sum_{k=0}^J e^{\delta_{kt}} \\
log \left( \frac{s_{jt}}{s_{0t}}\right ) &= \delta_{jt} \equiv u(X_t;\beta)
\end{aligned}
$$
Our next step would be to estimate the map between $X_t$ and $\delta_t$. This is another avenue in which our model can, and is in fact likely to fail. If we assume $X_t$ contains $K$ attributes, under standard restrictions on the colinearity of the characteristics, $X_t$ will span $\mathbb R^K$ and we will find a solution $\hat\beta$ that is exact. The issue arises when we consider multiple choice environments. Each of these choice environments can also be inverted to find an exact solution $\hat\beta_{t}$. Under the model, $\beta = \hat\beta_t = \hat\beta_{t'}, \forall t,t'$. However, because of sampling, informational, and misspecification error, the solutions are likely to differ across choice environments. What do we do when these maps are different, i.e. $\beta_t \neq \beta_{t'}$?

The solution to this problem is to define error in our estimation as 
$$ \delta_{t} = u(X_t;\hat\beta) + \xi_t $$
And choose $\hat\beta$ that minimizes the total contribution of $\xi_t$ across all markets. This estimation strategy has the additional benefit of being able to incorporate techniques to deal with misspecification error such as instrumental variables. 

# Models with Unobserved Hetrogeity

While we have a method for estimation, the model can preform poorly when we observe more than one choice environment for reasons other that $\xi$. Berry, Levinsohn \& Pakes (1995) give an illustrative example of this for the automobile industry. Consider that a Yugo (an inexpensive car) and a Mercedes (an expensive car) have the same choice probabilities. The model above enforces that these two cars have the same *cross-price derivative* for any third car. What does that mean? Suppose that we wish to predict the choice probabilities in a new choice setting $t'$ where a BMW (an expensive car) has a higher price (something all agents dislike) relative to its price in choice environment $t$. In comparison to $t$, we would expect that agents would substitute from the BMW towards the Mercedes, and the choice probability of the Yugo would remain unaffected. However, because the current model enforces that the Yugo and the Mercedes have the same cross-price derivative, our predicted choice probabilities of the Yugo and the Mercedes would both increase an equal amount in $t'$.

As a model of choice environments, this is an unattractive feature. We would like our model to capture the idea that given a change in the utility of one alternative, agents substitution patterns reflect similarity between the alternatives. A common way to introduce substitution patterns of this sort is through the random coefficient model given as
$$ u_{it} = u(X_t;\beta_i) + \epsilon_i $$
Where the map between the menu in choice environment $t$ is now tied to the agent through $\beta_i$, allowing for similar alternatives to have correlated utility evaluations. While this accomplishes our goal of realistic substitution patterns, it comes at the cost of the convenient bijective property of the previous model. This means that there is no closed-form solution.   

We lose the closed form solution because we now have a second expectation over the distribution of $\beta_i$. A convenient way to think about this distribution is to separate the stochastic part from the deterministic part and write $\beta_i = \tilde \beta + \nu_i$. We can now think about a deterministic vector $\tilde \beta$ and a random vector $\nu_i$ that follows some mean zero distribution $F_\nu$.

Using this notation, we can look at an expression involving both expectations. Note that under mild and non-economic restrictions, the order in which we take the expectations does not matter.
$$
\begin{aligned}
\tilde \delta_t &= \mathbb E_\epsilon \left [ \mathbb E_\nu [u_{it}] \right ]\\
&= \mathbb E_\epsilon \left [ \mathbb E_\nu [ u(X_t;\beta_i) + \epsilon_i ] \right ] \\
&= \mathbb E_\epsilon \left [ \mathbb E_\nu [ u(X_t;\tilde \beta+\nu_i)+\epsilon_i ] \right ] \\
&= \mathbb E_\epsilon\left[ \mathbb E_\nu [ u(X_t;\tilde \beta+\nu_i)]\right ] + \mathbb E_\epsilon\left[ \mathbb E_\nu [\epsilon_i]\right ] \\
&= \mathbb E_\nu\left[u(X_t;\tilde \beta + \nu_i) \right ]
\end{aligned}
$$

As $\epsilon$ enters our model additively, the linearity of the expectation allows us to integrate it out. We can contrast this with $\nu$, which does not enter additively. Because the unobserved heterogeneity does not enter additively, we are allowing for it to be non-orthogonal to the alteratives characteristics. 

Bu what is the expression in the expectation? More importantly, how would we take a sample analog? Lets again look at these expectations through their infinite sum representation that we introduced earlier, with the promise that it would come in handy.

$$ \tilde \delta_t = \lim_{N \to \infty} \frac{1}{N}\sum_{i=1}^{N} u(X_t;\tilde \beta + \nu_i)+\epsilon_{i} $$
As this is an infinite sum and $\epsilon_i$ is independent of $\nu_i$, we can group this summation in whatever way we please. We wish to group together all terms that share a common $\nu_i$. We can rewrite the summation as follows to emphasize this process. Let $R^N_b$ be a grouping of close by $\nu$ values.  Each $R^N_b$ can be thought of as a bin. Let the number of bins $B$ grow in proportion to the sample size $N$ such that in the limit as $N \to \infty$, each $R^N_b$ constitutes a point value. Let $|R_b^N|$ be the number of values in each unique bin, which is dependent on the implied bin size, which is in turn dependent on $N$. The sum is then of the form  
$$ \tilde \delta_t =\lim_{N \to \infty}\frac{1}{N} \left(\left[|R^N_1| \cdot \lim_{N_\epsilon \to \infty} \frac{1}{N_\epsilon} \sum_{i=1}^{N_\epsilon} u(X_t;\tilde \beta + \nu_1)  + \epsilon_i \right] +\dots + \left[|R^N_B|\cdot \lim_{N_\epsilon \to \infty} \frac{1}{N_\epsilon} \sum_{i=1}^{N_\epsilon} u(X_t;\tilde \beta+\nu_B)  + \epsilon_i \right]  \right) $$
Note that these two sums are equivalent as long as $F_\epsilon$ is independent $F_\nu$. As $F_\epsilon$ is assumed mean zero, the individual terms in that sum converge over $F_\epsilon$ such that

$$ \lim_{N_\epsilon \to \infty} \frac{1}{N_\epsilon} \sum_{i=1}^{N_\epsilon} u(X_t;\tilde \beta+\nu_i)  + \epsilon_i \xrightarrow{d} u(X_t;\tilde \beta+\nu_i), \forall b $$
This allows us to again rewrite the larger summation as

$$ 
\begin{aligned}
\tilde \delta_t &=\lim_{N \to \infty}\frac{1}{N} \left(\left[|R^N_1| \cdot u(X_t;\tilde \beta+\nu_1) \right] +\dots + \left[|R^N_B| \cdot u(X_t;\tilde \beta+\nu_B)  \right]  \right) \\
&= \lim_{N\to\infty} \sum_{b=1}^N \frac{|R^N_b|}{N} u(X_t;\tilde \beta+\nu_i) \\
&= \int_\Omega u(X_t;\tilde \beta + \nu_i)f_\nu(z) \,dz
\end{aligned}
$$
Where the final equality comes from $\lim_{N\to \infty} \frac{|R_b^N|}{N} = f_\nu$, where $f_\nu$ is the (non-parametric) density of the unobserved heterogeneity with support $\Omega$.  

We can then evaluate this integral on a grid. Assume we have $\overline R\to \infty$ grid points $\{\beta_r\}$ distributed through $\mathbb R^K$. Let $\theta^r = f_\nu(\beta_r)$. Take evaluations of the utility function, $u(X_t;\beta_r$) at each point on this grid. As $\overline R\to \infty$  we maintain equality and have
$$ \tilde \delta_t =  \int_\Omega u(X_t;\tilde \beta + \nu_i)f_\nu(z) \,dz = \lim_{\overline R\to\infty} \sum_{r=1}^{\overline R} \theta^r u(X_t;\beta_r) $$
Using our standard inversion formula, we have the infeasible estimating equation
$$ \tilde \delta_{jt} = \frac{s_{jt}}{s_{0t}} = \lim_{\overline R\to\infty} \sum_{r=1}^{\overline R} \theta^r u(X_t;\beta_r) $$
To make this estimating equation feasible, we use a finite, yet large, $R$. As the set of true $\{\theta\}$ are defined as the density $f_\nu$, our estimation procedure requires that our recovered $\{\hat\theta\}$ behave as a density as well. This means that $\hat\theta_r \geq 0$ and $\sum_{r=1}^R \hat\theta^r = 1$. This can easily be accomplished via constrained least squares and the following estimating equation.  
$$ \delta_{jt} = \sum_{r=1}^R \hat\theta^r u(X_t;\beta_r) + u_{jt} $$
### Code Example

Here we will generate an exotic distribution for $f_\nu$, and recover that distribution through a discrete choice framework. Our ``true" distribution will be the mixture of two, two dimensional, normal distributions given as 
$$ g_1 \sim \mathcal N(\begin{pmatrix} 2 \\ 2 \end{pmatrix},\begin{pmatrix} 1 & .5 \\ .5 & 1\end{pmatrix}), g_2 \sim \mathcal N(\begin{pmatrix} -1 \\ 1 \end{pmatrix},\begin{pmatrix} 1 & -.5 \\ -.5 & 1\end{pmatrix}) $$
Along with a confounding distribution that only enters
With mixture weights given as

$$ f_\nu = \frac{1}{2} g_1 + \frac{1}{4}g_2 + \frac{1}{4} g_3 $$
We can then look simulate from this distribution and view are 
```{r}
require(MASS)
require(ggplot2)
K = 2

drawF_nu = function(N){
.5*mvrnorm(N,c(5,5),rbind(c(1,.5),c(.5,1)))+.25*mvrnorm(N,c(0,1),rbind(c(1,-.5),c(-.5,1)))+.25*mvrnorm(N,c(0,-1),rbind(c(2,-1),c(-1,1)))
}
N = 10000
ggdf = data.frame(drawF_nu(N))
ggplot(ggdf,aes(x = X1, y = X2)) + geom_density_2d_filled()

```

 





