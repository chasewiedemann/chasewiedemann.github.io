---
title: "Replication"
output: html_document
---
```{r,echo = F}
base_dir = getwd()
base_url = paste0("https://",substr(base_dir,8, nchar(base_dir)))
wp_link = paste0(base_url,"/WorkingPaper.html")
ws_link = paste0(base_url,"/WorkingSlides.html")
rep_link = paste0(base_url,"/Replication.html")
miniblog_link = paste0(base_url,"/MiniBlog.html")
```

[Home](https://chasewiedemann.github.io/index.html) |
[Projects](https://chasewiedemann.github.io/Site/Projects/projects.html) |
[Blog](https://chasewiedemann.github.io/Site/blog.html) |

[Working Paper](`r wp_link`)|
[Working Slides](`r ws_link`)|
[Replication](`r rep_link`)|


# Models without Unobserved Heterogenity 

Consider a simple discrete choice model where the agents share a common utility function $u(X_t;\beta)$ and make choices over $J+1$ alternatives. The menu $X_t$ contains the characteristics of the alternatives in choice setting $t$ that the researcher believes effect utility and $s_t$ is a vector of choice probabilities that the researcher has collected. In all settings of interest this extremely simple model will be rejected by our data. Given that agents share a common utility function, they will all value the alternatives the same, and will all choose the same alternative to maximize utility. Contrast that with our observation that $s_t$ contains $J+1$ alternatives are present and likely has more than one positive choice probability.

The logical next step is to add an idiosyncratic preference shock to our agents utility function. This allows us to model agent $i$'s vector of utility for the alternatives as
$$ u_{it} = u(X_t;\beta) + \epsilon_i $$
Where $\epsilon_i$ is a $J\times 1$ random vector from a distribution $F_\epsilon$. Integrating out $\epsilon$ by taking an expectation over $F_\epsilon$ gives the mean utility $\delta_t$. We assume that $F_\epsilon$ is mean zero and orthogonal to the characteristics of the alternatives. Therefore we have
$$ \delta_t = \mathbb E_\epsilon\left[u_{it}\right] = u(X_t;\beta) $$
Another way to view the expectation taken over $F_\epsilon$ is through the infinite sum
$$ \delta_t = \lim_{N\to\infty} \frac{1}{N}\sum_{i=1}^N u_{it} + \epsilon_i $$
To transition from mean utility $\delta_t$ to choice probabilities, we can incorporate a choice function into the infinite sum. Let $\gamma:\mathbb R^{J+1} \to \{0,1\}^{J+1}$ be such a choice function, i.e. let $\gamma_i(x) = 1 \iff x_i = \max_{0\leq i \leq J} x_i$ and 0 otherwise. The choice function is incorporated in the infinte sum as follows

$$ s_t = \lim_{N\to\infty} \frac{1}{N}\sum_{i=1}^N \gamma(u_{it} + \epsilon_i) $$

Two important properties of the choice function are worth examining. First, the choice function is invariant to to location. Consider a constant vector $\mu \in \mathbb R^{J+1}$, meaning $\mu_i = \mu_j, \forall i,j$.  Note that $\gamma(x) = \gamma(x+\mu)$ for all constant vectors $\mu$. This is because the choice function is only concerned with the ordinal ranking of the utilities, and a change in location does not effect ordinal rankings. Second, the choice function is invariant to positive scaling. Letting $\sigma \in \mathbb R_{++}$ be a positive scalar, we see that $\gamma(x) = \gamma(\sigma \cdot x)$. This second property differs from the first property in the fact that for a fixed utility level $u_i$ and location $\mu$, the resulting choice probabilities associated with the infinite sum depends on the choice of $\sigma$. This is because as $\sigma\to \infty$, the relative differences between components in $u_i$ tend to matter less and less, and $s_t$ tends towards the constant market share value, $\overline s_j = \frac{1}{J+1}, \forall j$. Conversely, as $\sigma\to 0$, the relative differences between components in $u_i$ become more and more dominate, and in the limit we are back to the case without any idiosyncratic shocks to preferences. 

Given a normalization on $\mu$ and $\sigma$, Berry (1994) shows that there is a bijection between mean utilities $\delta_t$ and any vector of choice probabilities (including the one we observe) $s_t$. This allows us to take any vector of choice probabilities $s_t$ and associate them with a unique value of $\delta_t$. We can then map $X_t$ onto $\delta_t$ to recover $\beta$.

This bijection is facilitated by the distribution function $F_\epsilon$. Distribution functions can be seen as a mapping from the triple $(\delta,\mu,\sigma)$ into the space of choice probabilities. A more intuitive way to examine this mapping is through the inverse mapping from the choice probabilities into $(\delta,\mu,\sigma)$. The inverse maps of distribution functions are called quantile functions. Without normalization, quantile functions are not functions at all, but rather a correspondence between $s_t$ and the triple. What does this mean? For any observed choice probability $s_0$ and any mean utility $\delta_0$, there exists $(\mu_0,\sigma_0)$ such that the quantile function will associate the $s_0$ with $\delta_0$, i.e. $F_\epsilon^{-1} (s_0,\mu^*,\sigma^*) = \delta_0$. Further, $(\mu_0,\sigma_0)$ are not themselves unique.  Another way of stating this same property is that the quantile function spans $\mathbb R^{J+1}$ if we allow $\sigma$ and $\mu$ to vary. Normalizations pin down the values of $\mu$ and $\sigma$, which restrict the quantile function to be a bijective function, facilitating inversion. 



Now choose a specific choice probability $s_0$, fix $\sigma=\sigma_0$ and let $\mu$ vary. The quantile function now maps $s_0$ into a subset of $\mathbb R^{J+1}$ instead of the entire space as before. We can interpolate through the subset defined by $s_0$ and $\sigma$ with our free parameter $\mu$. The graph of this interpolation can be seen as a line through $\mathbb R^{J+1}$, and changing $s_0$ while holding $\sigma_0$ fixed creates lines that combine to make the entire space $\mathbb R^{J+1}$. How does a specific $s_0$ with a fixed $\sigma_0$ get associated to a particular line? We can first examine the constant choice probability $\overline s$ introduced before. This choice probability tell us that on average, consumers are indifferent between all $J+1$ alternatives. It stands to reason that the mean utilities associated with these should be associated with the line that contains all constant utilities, which includes the origin. For example, with two choice alternatives, the line associated with $\overline s$ is simply the $45^\circ$ line. 

Lets now take a step of size $\Delta$ away from $\overline s$ in the first dimension. The resulting choice probability $s_0$ looks like 

$$ s_0 = \begin{pmatrix} \frac{1}{J+1} + \Delta \\ \frac{1}{J+1} - \frac{\Delta}{J} \\ \vdots \\ \frac{1}{J+1} - \frac{\Delta}{J} \end{pmatrix}  $$

Intuitively, the line of utilities that is interpolated by $\mu$ should be greater in the first dimension, and equal in the remaining dimensions, for all values of $\mu$. Again in the case where we have two alternatives, if the first dimension is plotted on the X-axis, then the line of utilities should be parallel and above the $45^\circ$ line. Three observations are important to note here. First, how the size of $\Delta$ corresponds to a change in the utilities is determined by the value of $\sigma$. For a fixed choice of $\Delta$, a large value for $\sigma$ will cause the line to shift further from the line of constant vectors, and a small value of $\sigma$ will cause the line to remain close to the constant vectors. Second, $\overline s$ is "special" in the sense that it is the only choice probability that coincides with its utility value. The "lines" that $\mu$ interpolates through must cover the entirety of $\mathbb R^{J+1}$, so they expand outside the simplex that contains choice probabilities. Third, consider a hypersphere centered at $\overline s$. As the direction of the lines are fixed, the hypersphere will have exactly $J+1$ lines that are exactly tangent. These choice probabilities are related, and are simply permutations.     


To start gaining intuition, we can view these normalizations within the infinite sum

$$ s_t = \lim_{N\to\infty} \frac{1}{N}\sum_{i=1}^N \gamma\big(u_{it} + \mu   +\sigma\epsilon_i \big) $$ 
The interaction between $\epsilon$ and the normalizations become transparent in this form. The assumption that $F_\epsilon$ was mean zero can just as easily be viewed as the mean being captured by the $\mu$ term. As the choice function $\gamma$ is invariant to location, this assumption has no real empirical content, so $\mu$ can be seen as a free parameter. The $\sigma$ normalization, notation appropriately chosen, acts as the variance of the idiosyncratic shock. From this line of reasoning, we can always choose the standard distribution function (mean zero and variance one), and then allow normalizations to impact the mapping. 

A simple way to choose this canonical member, and therefore create a bijection that allows us to invert this model for $\delta_t$, is to normalize the mean utility of one alternative to zero.  While any of the alternatives can be normalized, the common way this is preformed is to normalize the mean utility of the outside alternative. The outside alternative is the alternative where our agent did not choose any of the other alternatives. If we were studying a choice environment regarding the market for transportation services, this alternative would capture all agents who decided to walk or bike. Let $j=0$ be the index of the outside alternative. This normalization selects exactly one member of each equivalence class because $\delta_{0t} + \alpha_0 = 0$ for exactly one value of $\alpha$. 

This bijection holds for all choices of $F_\epsilon$. However, assuming that $F_\epsilon$ is just $J$ copies of the single dimensional Type-1 Extreme Value distribution allows for this bijection to have a closed form solution given by
$$ s_{jt} = \frac{e^{\delta_{jt}}}{\sum_{k=0}^J e^{\delta_{kt}}} $$
This follows from properties of that (univariate) Type-1 Extreme Value distribution and a derivation can be found in Train (2002). Then, using our conveniently chosen normalization, we can write
$$ s_{0t} = \frac{e^{\delta_{0t}}}{\sum_{k=0}^J e^{\delta_{kt}}} = \frac{1}{\sum_{k=0}^J e^{\delta_{kt}}} $$
We see that this normalization has given us the value of the denominator of our expression. We can then divide by the value of this denominator and take the natural log to yield $\delta_{jt}$ as
$$ 
\begin{aligned}
\frac{s_{jt}}{s_{0t}} &= \frac{e^{\delta_{jt}}}{\sum_{k=0}^J e^{\delta_{kt}}} \cdot \sum_{k=0}^J e^{\delta_{kt}} \\
log \left( \frac{s_{jt}}{s_{0t}}\right ) &= \delta_{jt} \equiv u(X_t;\beta)
\end{aligned}
$$
Our next step would be to estimate the map between $X_t$ and $\delta_t$. This is another avenue in which our model can, and is in fact likely to fail. If we assume $X_t$ contains $K$ attributes, under standard restrictions on the colinearity of the characteristics, $X_t$ will span $\mathbb R^K$ and we will find a solution $\hat\beta$ that is exact. The issue arises when we consider multiple choice environments. Each of these choice environments can also be inverted to find an exact solution $\hat\beta_{t}$. Under the model, $\beta = \hat\beta_t = \hat\beta_{t'}, \forall t,t'$. However, because of sampling, informational, and misspecification error, the solutions are likely to differ across choice environments. What do we do when these maps are different, i.e. $\beta_t \neq \beta_{t'}$?

The solution to this problem is to define error in our estimation as 
$$ \delta_{t} = u(X_t;\hat\beta) + \xi_t $$
And choose $\hat\beta$ that minimizes the total contribution of $\xi_t$ across all markets. This estimation strategy has the additional benefit of being able to incorporate techniques to deal with misspecification error such as instrumental variables. 

# Models with Unobserved Hetrogeity

While we have a method for estimation, the model can preform poorly when we observe more than one choice environment for reasons other that $\xi$. Berry, Levinsohn \& Pakes (1995) give an illustrative example of this for the automobile industry. Consider that a Yugo (an inexpensive car) and a Mercedes (an expensive car) have the same choice probabilities. The model above enforces that these two cars have the same *cross-price derivative* for any third car. What does that mean? Suppose that we wish to predict the choice probabilities in a new choice setting $t'$ where a BMW (an expensive car) has a higher price (something all agents dislike) relative to its price in choice environment $t$. In comparison to $t$, we would expect that agents would substitute from the BMW towards the Mercedes, and the choice probability of the Yugo would remain unaffected. However, because the current model enforces that the Yugo and the Mercedes have the same cross-price derivative, our predicted choice probabilities of the Yugo and the Mercedes would both increase an equal amount in $t'$.

As a model of choice environments, this is an unattractive feature. We would like our model to capture the idea that given a change in the utility of one alternative, agents substitution patterns reflect similarity between the alternatives. A common way to introduce substitution patterns of this sort is through the random coefficient model given as
$$ u_{it} = u(X_t;\beta_i) + \epsilon_i $$
Where the map between the menu in choice environment $t$ is now tied to the agent through $\beta_i$, allowing for similar alternatives to have correlated utility evaluations. While this accomplishes our goal of realistic substitution patterns, it comes at the cost of the convenient bijective property of the previous model. This means that there is no closed-form solution.   

We lose the closed form solution because we now have a second expectation over the distribution of $\beta_i$. A convenient way to think about this distribution is to separate the stochastic part from the deterministic part and write $\beta_i = \tilde \beta + \nu_i$. We can now think about a deterministic vector $\tilde \beta$ and a random vector $\nu_i$ that follows some mean zero distribution $F_\nu$.

Using this notation, we can look at an expression involving both expectations. Note that under mild and non-economic restrictions, the order in which we take the expectations does not matter.
$$
\begin{aligned}
\tilde \delta_t &= \mathbb E_\epsilon \left [ \mathbb E_\nu [u_{it}] \right ]\\
&= \mathbb E_\epsilon \left [ \mathbb E_\nu [ u(X_t;\beta_i) + \epsilon_i ] \right ] \\
&= \mathbb E_\epsilon \left [ \mathbb E_\nu [ u(X_t;\tilde \beta+\nu_i)+\epsilon_i ] \right ] \\
&= \mathbb E_\epsilon\left[ \mathbb E_\nu [ u(X_t;\tilde \beta+\nu_i)]\right ] + \mathbb E_\epsilon\left[ \mathbb E_\nu [\epsilon_i]\right ] \\
&= \mathbb E_\nu\left[u(X_t;\tilde \beta + \nu_i) \right ]
\end{aligned}
$$

As $\epsilon$ enters our model additively, the linearity of the expectation allows us to integrate it out. We can contrast this with $\nu$, which does not enter additively. Because the unobserved heterogeneity does not enter additively, we are allowing for it to be non-orthogonal to the alteratives characteristics. 

Bu what is the expression in the expectation? More importantly, how would we take a sample analog? Lets again look at these expectations through their infinite sum representation that we introduced earlier, with the promise that it would come in handy.

$$ \tilde \delta_t = \lim_{N \to \infty} \frac{1}{N}\sum_{i=1}^{N} u(X_t;\tilde \beta + \nu_i)+\epsilon_{i} $$
As this is an infinite sum and $\epsilon_i$ is independent of $\nu_i$, we can group this summation in whatever way we please. We wish to group together all terms that share a common $\nu_i$. We can rewrite the summation as follows to emphasize this process. Let $R^N_b$ be a grouping of close by $\nu$ values.  Each $R^N_b$ can be thought of as a bin. Let the number of bins $B$ grow in proportion to the sample size $N$ such that in the limit as $N \to \infty$, each $R^N_b$ constitutes a point value. Let $|R_b^N|$ be the number of values in each unique bin, which is dependent on the implied bin size, which is in turn dependent on $N$. The sum is then of the form  
$$ \tilde \delta_t =\lim_{N \to \infty}\frac{1}{N} \left(\left[|R^N_1| \cdot \lim_{N_\epsilon \to \infty} \frac{1}{N_\epsilon} \sum_{i=1}^{N_\epsilon} u(X_t;\tilde \beta + \nu_1)  + \epsilon_i \right] +\dots + \left[|R^N_B|\cdot \lim_{N_\epsilon \to \infty} \frac{1}{N_\epsilon} \sum_{i=1}^{N_\epsilon} u(X_t;\tilde \beta+\nu_B)  + \epsilon_i \right]  \right) $$
Note that these two sums are equivalent as long as $F_\epsilon$ is independent $F_\nu$. As $F_\epsilon$ is assumed mean zero, the individual terms in that sum converge over $F_\epsilon$ such that

$$ \lim_{N_\epsilon \to \infty} \frac{1}{N_\epsilon} \sum_{i=1}^{N_\epsilon} u(X_t;\tilde \beta+\nu_i)  + \epsilon_i \xrightarrow{d} u(X_t;\tilde \beta+\nu_i), \forall b $$
This allows us to again rewrite the larger summation as

$$ 
\begin{aligned}
\tilde \delta_t &=\lim_{N \to \infty}\frac{1}{N} \left(\left[|R^N_1| \cdot u(X_t;\tilde \beta+\nu_1) \right] +\dots + \left[|R^N_B| \cdot u(X_t;\tilde \beta+\nu_B)  \right]  \right) \\
&= \lim_{N\to\infty} \sum_{b=1}^N \frac{|R^N_b|}{N} u(X_t;\tilde \beta+\nu_i) \\
&= \int_\Omega u(X_t;\tilde \beta + \nu_i)f_\nu(z) \,dz
\end{aligned}
$$
Where the final equality comes from $\lim_{N\to \infty} \frac{|R_b^N|}{N} = f_\nu$, where $f_\nu$ is the (non-parametric) density of the unobserved heterogeneity with support $\Omega$.  

We can then evaluate this integral on a grid. Assume we have $\overline R\to \infty$ grid points $\{\beta_r\}$ distributed through $\mathbb R^K$. Let $\theta^r = f_\nu(\beta_r)$. Take evaluations of the utility function, $u(X_t;\beta_r$) at each point on this grid. As $\overline R\to \infty$  we maintain equality and have
$$ \tilde \delta_t =  \int_\Omega u(X_t;\tilde \beta + \nu_i)f_\nu(z) \,dz = \lim_{\overline R\to\infty} \sum_{r=1}^{\overline R} \theta^r u(X_t;\beta_r) $$
Using our standard inversion formula, we have the infeasible estimating equation
$$ \tilde \delta_{jt} = \frac{s_{jt}}{s_{0t}} = \lim_{\overline R\to\infty} \sum_{r=1}^{\overline R} \theta^r u(X_t;\beta_r) $$
To make this estimating equation feasible, we use a finite, yet large, $R$. As the set of true $\{\theta\}$ are defined as the density $f_\nu$, our estimation procedure requires that our recovered $\{\hat\theta\}$ behave as a density as well. This means that $\hat\theta_r \geq 0$ and $\sum_{r=1}^R \hat\theta^r = 1$. This can easily be accomplished via constrained least squares and the following estimating equation.  
$$ \delta_{jt} = \sum_{r=1}^R \hat\theta^r u(X_t;\beta_r) + u_{jt} $$
### Code Example

Here we will generate an exotic distribution for $f_\nu$, and recover that distribution through a discrete choice framework. Our ``true" distribution will be the mixture of two, two dimensional, normal distributions given as 
$$ g_1 \sim \mathcal N(\begin{pmatrix} 2 \\ 2 \end{pmatrix},\begin{pmatrix} 1 & .5 \\ .5 & 1\end{pmatrix}), g_2 \sim \mathcal N(\begin{pmatrix} -1 \\ 1 \end{pmatrix},\begin{pmatrix} 1 & -.5 \\ -.5 & 1\end{pmatrix}) $$
Along with a confounding distribution that only enters
With mixture weights given as

$$ f_\nu = \frac{1}{2} g_1 + \frac{1}{4}g_2 + \frac{1}{4} g_3 $$
We can then look simulate from this distribution and view are 
```{r}
require(MASS)
require(ggplot2)
K = 2

drawF_nu = function(N){
.5*mvrnorm(N,c(5,5),rbind(c(1,.5),c(.5,1)))+.25*mvrnorm(N,c(0,1),rbind(c(1,-.5),c(-.5,1)))+.25*mvrnorm(N,c(0,-1),rbind(c(2,-1),c(-1,1)))
}
N = 10000
ggdf = data.frame(drawF_nu(N))
ggplot(ggdf,aes(x = X1, y = X2)) + geom_density_2d_filled()

```

 





